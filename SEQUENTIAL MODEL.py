# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmj5fdKCLC6OvH8qFOlTjMSJuEwJxc5Y
"""

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/Dataset'

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

image_size = (224, 224)
batch_size = 32

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='validation'
)

import os
classes = os.listdir(dataset_path)
for class_name in classes:
    class_folder = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_folder):
        num_images = len([
            file for file in os.listdir(class_folder)
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))
        ])
        print(f"{class_name}: {num_images} images")

import os
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img

# Parameters
input_dir = dataset_path
output_dir = "augmented_dataset"
target_count_per_class = 2500  # aim to get 2500 per class (for 2 classes = 5000 total)
image_size = (224, 224)

# Augmentation setup
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Create output directories
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
shutil.copytree(input_dir, output_dir)

for class_name in os.listdir(input_dir):
    class_input_path = os.path.join(input_dir, class_name)
    class_output_path = os.path.join(output_dir, class_name)

    existing_images = [f for f in os.listdir(class_input_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    image_count = len(existing_images)

    print(f"Augmenting class '{class_name}': {image_count} images -> {target_count_per_class} target")

    i = 0
    while image_count + i < target_count_per_class:
        for image_name in existing_images:
            img_path = os.path.join(class_input_path, image_name)
            img = load_img(img_path, target_size=image_size)
            x = img_to_array(img)
            x = x.reshape((1,) + x.shape)

            # Generate 1 augmented image at a time
            for batch in datagen.flow(x, batch_size=1, save_to_dir=class_output_path,
                                      save_prefix='aug', save_format='jpg'):
                i += 1
                if image_count + i >= target_count_per_class:
                    break

    print(f"âœ“ Class '{class_name}' augmented to {target_count_per_class} images")

data = '/content/augmented_dataset'

import os
classes = os.listdir(data)
for class_name in classes:
    class_folder = os.path.join(data, class_name)
    if os.path.isdir(class_folder):
        num_images = len([
            file for file in os.listdir(class_folder)
            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))
        ])
        print(f"{class_name}: {num_images} images")

image_size = (224, 224)
batch_size = 32

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_generator = datagen.flow_from_directory(
    data,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    data,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='validation'
)

model = models.Sequential([
    layers.Input(shape=(224, 224, 3)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')  # Binary classification
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator
)

model.save("accident_classifier_model.h5")

loss, accuracy = model.evaluate(val_generator)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

